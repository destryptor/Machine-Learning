{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "78ae8209-59d7-465d-9193-b0a5f23109f2",
   "metadata": {},
   "source": [
    "### Problem Statement\r\n",
    "Estimate individual medical insurance expenses based on factors such as age, BMI, smoking status, and other health-related variables.\r\n",
    "\r\n",
    "### Data Description\r\n",
    "**Dataset Filename:** `data_insurance.csv`\r\n",
    "\r\n",
    "#### Attributes Information:\r\n",
    "1. **age:** Age of the primary policyholder.\r\n",
    "2. **sex:** Gender of the insurance policyholder (female or male).\r\n",
    "3. **bmi:** Body Mass Index, a measure that uses the ratio of weight to height (kg/mÂ²) to categorize body weight, ideally ranging from 18.5 to 24.9.\r\n",
    "4. **children:** Number of dependents covered by the health insurance.\r\n",
    "5. **smoker:** Indicates if the person smokes.\r\n",
    "6. **region:** The geographical area in the U.S. where the policyholder resides (northeast, southeast, southwest, northwest).\r\n",
    "\r\n",
    "**Target:**\r\n",
    "- **charges:** Medical expenses billed to the insurance for the individual.\r\n",
    "vidual.\r",
    "rice of unit area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c2d849f0-1b2a-4b46-95a3-ebd2b3b9189e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import math\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18928b1c-12a1-4e2f-b963-cc4003a39f3b",
   "metadata": {},
   "source": [
    "### Reading the data\n",
    "Here we read the data from the file and store it in a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d11bd93d-9d47-4345-8344-bb78b542d1f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = 'data_insurance.csv'\n",
    "\n",
    "df = pd.read_csv(filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11860af7-c448-4eb5-a71b-ed8a2f5581bf",
   "metadata": {},
   "source": [
    "### Explore the data\n",
    "We explore the first few rows of the dataset and describe it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "25abded8-fd85-493b-998f-e7aea0ea76bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First few rows of the dataset:\n",
      "   age     sex     bmi  children smoker     region      charges\n",
      "0   19  female  27.900         0    yes  southwest  16884.92400\n",
      "1   18    male  33.770         1     no  southeast   1725.55230\n",
      "2   28    male  33.000         3     no  southeast   4449.46200\n",
      "3   33    male  22.705         0     no  northwest  21984.47061\n",
      "4   32    male  28.880         0     no  northwest   3866.85520\n",
      "\n",
      "Summary statistics:\n",
      "               age          bmi     children       charges\n",
      "count  1338.000000  1338.000000  1338.000000   1338.000000\n",
      "mean     39.207025    30.663397     1.094918  13270.422265\n",
      "std      14.049960     6.098187     1.205493  12110.011237\n",
      "min      18.000000    15.960000     0.000000   1121.873900\n",
      "25%      27.000000    26.296250     0.000000   4740.287150\n",
      "50%      39.000000    30.400000     1.000000   9382.033000\n",
      "75%      51.000000    34.693750     2.000000  16639.912515\n",
      "max      64.000000    53.130000     5.000000  63770.428010\n",
      "\n",
      "Data types of each column:\n",
      "age           int64\n",
      "sex          object\n",
      "bmi         float64\n",
      "children      int64\n",
      "smoker       object\n",
      "region       object\n",
      "charges     float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(\"First few rows of the dataset:\")\n",
    "print(df.head())\n",
    "\n",
    "print(\"\\nSummary statistics:\")\n",
    "print(df.describe())\n",
    "\n",
    "print(\"\\nData types of each column:\")\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2afe6e99-39ca-4cb8-badc-75195fab06ef",
   "metadata": {},
   "source": [
    "### Handling missing data\n",
    "1. Option 1: Fill missing values with a specific value (e.g., mean)\n",
    "2. Option 2: Drop rows/columns with missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "93125e5b-a2f2-4978-b223-3177ee2b5842",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Missing values in each column:\n",
      "age         0\n",
      "sex         0\n",
      "bmi         0\n",
      "children    0\n",
      "smoker      0\n",
      "region      0\n",
      "charges     0\n",
      "dtype: int64\n",
      "\n",
      "No missing values found in the dataset.\n"
     ]
    }
   ],
   "source": [
    "missing_values = df.isnull().sum()\n",
    "print(\"\\nMissing values in each column:\")\n",
    "print(missing_values)\n",
    "\n",
    "if missing_values.sum() > 0:\n",
    "    df.fillna(df.mean(), inplace=True)\n",
    "    print(\"\\nMissing values were found and have been filled with the mean of each column.\")\n",
    "\n",
    "    #df.dropna(inplace=True)\n",
    "    #print(\"\\nMissing values were found and have been dropped.\")\n",
    "    \n",
    "    print(\"\\nMissing values after handling:\")\n",
    "    print(df.isnull().sum())\n",
    "else:\n",
    "    print(\"\\nNo missing values found in the dataset.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29078d7d-0e2c-4fa0-a490-dec99be40cb6",
   "metadata": {},
   "source": [
    "### Converting Categorical Variables\n",
    "We convert non-numeric features like 'sex', 'smoker' and 'region' into numerical values using encoding techniques.\n",
    "\n",
    "Since 'sex' and 'smoker' have binary values, we use **label_encoding** on them. \\\n",
    "'region' has multiple values, hence we use **one-hot encoding**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "370deecb-4c76-4a79-85ae-cf1f53a154d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset after encoding categorical variables:\n",
      "   age  sex     bmi  children  smoker  region      charges\n",
      "0   19    0  27.900         0       1       0  16884.92400\n",
      "1   18    1  33.770         1       0       1   1725.55230\n",
      "2   28    1  33.000         3       0       1   4449.46200\n",
      "3   33    1  22.705         0       0       2  21984.47061\n",
      "4   32    1  28.880         0       0       2   3866.85520\n"
     ]
    }
   ],
   "source": [
    "label_encoding_map = {\n",
    "    'sex': {'female': 0, 'male': 1},\n",
    "    'smoker': {'no': 0, 'yes': 1},\n",
    "    'region': {'southwest': 0, 'southeast': 1, 'northwest': 2, 'northeast': 3}\n",
    "}\n",
    "\n",
    "df['sex'] = df['sex'].map(label_encoding_map['sex'])\n",
    "df['smoker'] = df['smoker'].map(label_encoding_map['smoker'])\n",
    "df['region'] = df['region'].map(label_encoding_map['region'])\n",
    "\n",
    "print(\"\\nDataset after encoding categorical variables:\")\n",
    "print(df.head())\n",
    "# print(df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d845021-6436-4541-97ef-8de91601341b",
   "metadata": {},
   "source": [
    "### Defining Features and Targets\n",
    "We create 2 numpy arrays $X$ and $y$ from the dataframe:\n",
    "+ $X$: Input data of the shape (number of samples, number of input features)\n",
    "+ $y$: Target variable of the shape (number of samples,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "23bfadd1-48ab-4732-90d1-e7df5c68bda3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X:  (1338, 6) , Shape of y:  (1338,)\n"
     ]
    }
   ],
   "source": [
    "X = df.drop(columns=['charges']).values\n",
    "y = df['charges'].values\n",
    "\n",
    "print(\"Shape of X: \", X.shape, \", Shape of y: \", y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b50e6d01-c97f-40ab-a758-f45de8062a42",
   "metadata": {},
   "source": [
    "### Splitting the Dataset\n",
    "We need to pre process the data. We use min-max scaler to scale the input data $X$.\n",
    "After that, we split the data (**X** and **y**) into a training dataset (**X_train** and **y_train**) and test dataset (**X_test** and **y_test**)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "30897cdd-9272-47d6-af8d-5f5a3d1786a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train:  (1003, 6) , Shape of y_train:  (1003,)\n",
      "Shape of X_test:  (335, 6) , Shape of y_test:  (335,)\n"
     ]
    }
   ],
   "source": [
    "def train_test_split(X, y, test_size=0.25, random_state=None):\n",
    "    if random_state is not None:\n",
    "        np.random.seed(random_state)\n",
    "    indices = np.arange(X.shape[0])\n",
    "    np.random.shuffle(indices)\n",
    "\n",
    "    split_index = int(X.shape[0] * (1 - test_size))\n",
    "\n",
    "    train_indices = indices[:split_index]\n",
    "    test_indices = indices[split_index:]\n",
    "\n",
    "    X_train = X[train_indices]\n",
    "    X_test = X[test_indices]\n",
    "    y_train = y[train_indices]\n",
    "    y_test = y[test_indices]\n",
    "\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "def min_max_scaler(X, feature_range=(0, 1)):\n",
    "    X_min = np.min(X, axis=0)\n",
    "    X_max = np.max(X, axis=0)\n",
    "\n",
    "    X_scaled = (X-X_min)/(X_max-X_min)\n",
    "\n",
    "    return X_scaled\n",
    "\n",
    "\n",
    "X = min_max_scaler(X)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "print(\"Shape of X_train: \",X_train.shape, \", Shape of y_train: \",y_train.shape)\n",
    "print(\"Shape of X_test: \",X_test.shape, \", Shape of y_test: \",y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a43cec-cb0c-4fa5-b92a-92855307a802",
   "metadata": {},
   "source": [
    "### Computing the loss function\n",
    "In linear regression, the model parameters are:\n",
    "\n",
    "+ $w$: Parameters of the linear regression model (excluding the bias) of the shape (1, number of features)\n",
    "\n",
    "+ $b$: Bias parameter (scalar) of the linear regression model\n",
    "\n",
    "Both $w$ and $b$ are numpy arrays.\n",
    "\n",
    "Given the model parameters $w$ and $b$, the prediction for an input sample $X^i$ is:\n",
    "$$h_{w,b}(X^i) = w \\cdot X^i + b$$\n",
    "where $X^i$ is the $i^{th}$ training sample with shape (number of features,1)\n",
    "\n",
    "We implement and compute Mean Squarred Error loss fucntion:\n",
    "$$ L_{w,b}(X) = (1/(2m))\\sum_{i=1}^{m}(y^i - h_{w,b}(X^i))^2 $$\n",
    "where $y^i$ is the true target value for the $i^{th}$ sample and $h_{w,b}(X^i)$ is the predicted value for the $i^{th}$ sample using the parameters $w$ and $b$.\n",
    "\n",
    "$w$ is the list of parameters excluding the bias and $b$ is the bias term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "79e29bde-a5dd-4c49-8471-48afd959b0ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(X, y, w, b):\n",
    "    m = X.shape[0]\n",
    "\n",
    "    loss = 0\n",
    "    for i in range(m):\n",
    "        loss += (y[i] - (np.dot(X[i], w) + b)) ** 2        \n",
    "\n",
    "    loss = (1 / (2 * m)) * loss\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b3a6e54-1b7c-445f-b80f-70c6134e154a",
   "metadata": {},
   "source": [
    "### Computing the Gradient of the Loss\n",
    "\n",
    "In this following function ```compute_gradient```, we compute the gradients $\\frac{\\partial L}{\\partial w}$ and $\\frac{\\partial L}{\\partial b}$ of the loss $L$ w.r.t. $w$ and $b$. More specifically, we iterate over every training example and compute the gradients of the loss for that training example. Finally, we aggregate the gradient values for all the training examples and take the average. The gradients can be computed as:\n",
    "$$\\frac{\\partial L}{\\partial w} = \\frac{1}{m} \\sum_{i=1}^m (h_{w,b}(X^i)-y^i)X^i$$\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial b} = \\frac{1}{m} \\sum_{i=1}^{m} (h_{w,b}(X^i)-y^i)$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0f7994ec-b254-4d4b-999a-b507ac1e3b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient(X, y, w, b):\n",
    "    m = X.shape[0]\n",
    "\n",
    "    dL_dw = None\n",
    "    dL_db = None\n",
    "\n",
    "    n = w.shape[0]\n",
    "    dL_dw = np.zeros((n, ))\n",
    "    dL_db = 0.0\n",
    "\n",
    "    for i in range(m):\n",
    "        diff = (np.dot(X[i], w) + b) - y[i]\n",
    "        for j in range(n):\n",
    "            dL_dw[j] += diff * X[i, j].item()\n",
    "        dL_db += diff\n",
    "\n",
    "    dL_dw /= m\n",
    "    dL_db /= m\n",
    "    return dL_dw, dL_db"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "250b08d7-3782-40fe-a8cf-734a2c7e780f",
   "metadata": {},
   "source": [
    "### Training the Model using Batch Gradient Descent\n",
    "We finally implement the batch gradient descent algorithm to train and learn the parameters of the linear regression model. We use the **loss_function** and **compute_gradient** functions implemented above.\n",
    "\n",
    "In this batch_gradient_descent function, we compute the gradient for the training samples and update the parameters $w$ and $b$ in every iteration:\n",
    "\n",
    "+ $w \\leftarrow w - \\alpha \\frac{\\partial L}{\\partial w}$\n",
    "\n",
    "+ $b \\leftarrow b - \\alpha \\frac{\\partial L}{\\partial b}$\n",
    "\n",
    "Additionally, we compute the loss function values in every iteration and store it in the list variable **loss_hist** and print the loss value after every 100 iterations during the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1564a0e8-57e6-4e9d-9c36-910cdec61bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_gradient_descent(X, y, w_initial, b_initial, alpha, num_iters):\n",
    "    m = X.shape[0]\n",
    "\n",
    "    loss_hist = []\n",
    "\n",
    "    w = copy.deepcopy(w_initial)\n",
    "    b = b_initial\n",
    "\n",
    "    for i in range(num_iters):\n",
    "        dL_dw, dL_db = compute_gradient(X, y, w, b)\n",
    "\n",
    "        w -= alpha * dL_dw\n",
    "        b -= alpha * dL_db\n",
    "\n",
    "        loss = loss_function(X, y, w, b)\n",
    "        loss_hist.append(loss)\n",
    "\n",
    "        if (i % 100 == 0):\n",
    "            print(f\"Iteration: {i} Loss: {loss}\")\n",
    "\n",
    "    return w, b, loss_hist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4359fa07-3dd4-443a-a7fb-e2dd8e38c1fd",
   "metadata": {},
   "source": [
    "### Initialising the parameters\n",
    "Now we intialize the model parameters ($w$ and $b$) and learning rate (**alpha**). The learning rate alpha is randomly initialized between 0.0001 and 0.001. For the learning rate, we set a seed and make use of the random function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d600a81d-5c0d-4124-a36e-ee8899ad7e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 88\n",
    "np.random.seed(random_seed)\n",
    "\n",
    "def initialize_parameters():\n",
    "    initial_w = None\n",
    "    initial_b = None\n",
    "    alpha = None\n",
    "\n",
    "    initial_w = np.random.random((X_train.shape[1], ))\n",
    "    initial_b = np.random.random()\n",
    "    alpha = random.uniform(0.0001, 0.001)\n",
    "\n",
    "    return initial_w,initial_b,alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "228248a5-3945-43c8-ad13-3ed8f77c52c7",
   "metadata": {},
   "source": [
    "### Training the model\n",
    "The model is trained using batch gradient descent algorithm for **num_iters=10000** iterations. You can change the number of iterations to check any improvements in the performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a2d13633-85cb-4f87-820c-2cd26c74805c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0 Loss: 163499555.73201534\n",
      "Iteration: 100 Loss: 149476863.18403253\n",
      "Iteration: 200 Loss: 137309708.49274582\n",
      "Iteration: 300 Loss: 126744941.69460116\n",
      "Iteration: 400 Loss: 117564039.72820069\n",
      "Iteration: 500 Loss: 109578368.97509564\n",
      "Iteration: 600 Loss: 102625095.96557187\n",
      "Iteration: 700 Loss: 96563657.56929278\n",
      "Iteration: 800 Loss: 91272714.12362745\n",
      "Iteration: 900 Loss: 86647519.42547262\n",
      "Iteration: 1000 Loss: 82597650.55245997\n",
      "Iteration: 1100 Loss: 79045048.282698\n",
      "Iteration: 1200 Loss: 75922325.61781953\n",
      "Iteration: 1300 Loss: 73171307.72819164\n",
      "Iteration: 1400 Loss: 70741771.65775506\n",
      "Iteration: 1500 Loss: 68590358.45794117\n",
      "Iteration: 1600 Loss: 66679634.15940874\n",
      "Iteration: 1700 Loss: 64977279.21802821\n",
      "Iteration: 1800 Loss: 63455388.85763186\n",
      "Iteration: 1900 Loss: 62089869.13695374\n",
      "Iteration: 2000 Loss: 60859915.64404705\n",
      "Iteration: 2100 Loss: 59747563.51332835\n",
      "Iteration: 2200 Loss: 58737299.00709292\n",
      "Iteration: 2300 Loss: 57815724.2384313\n",
      "Iteration: 2400 Loss: 56971267.76489822\n",
      "Iteration: 2500 Loss: 56193934.777033724\n",
      "Iteration: 2600 Loss: 55475091.464488454\n",
      "Iteration: 2700 Loss: 54807278.88367951\n",
      "Iteration: 2800 Loss: 54184052.29066756\n",
      "Iteration: 2900 Loss: 53599842.45518441\n",
      "Iteration: 3000 Loss: 53049835.948422454\n",
      "Iteration: 3100 Loss: 52529871.80865598\n",
      "Iteration: 3200 Loss: 52036352.34393599\n",
      "Iteration: 3300 Loss: 51566166.13767381\n",
      "Iteration: 3400 Loss: 51116621.58755481\n",
      "Iteration: 3500 Loss: 50685389.536654755\n",
      "Iteration: 3600 Loss: 50270453.75279491\n",
      "Iteration: 3700 Loss: 49870068.18237313\n",
      "Iteration: 3800 Loss: 49482720.05181415\n",
      "Iteration: 3900 Loss: 49107098.01659443\n",
      "Iteration: 4000 Loss: 48742064.667253464\n",
      "Iteration: 4100 Loss: 48386632.796292715\n",
      "Iteration: 4200 Loss: 48039944.91141418\n",
      "Iteration: 4300 Loss: 47701255.550954044\n",
      "Iteration: 4400 Loss: 47369916.01813104\n",
      "Iteration: 4500 Loss: 47045361.20318237\n",
      "Iteration: 4600 Loss: 46727098.20773724\n",
      "Iteration: 4700 Loss: 46414696.5248596\n",
      "Iteration: 4800 Loss: 46107779.56192511\n",
      "Iteration: 4900 Loss: 45806017.32261883\n",
      "Iteration: 5000 Loss: 45509120.0894738\n",
      "Iteration: 5100 Loss: 45216832.97006715\n",
      "Iteration: 5200 Loss: 44928931.18871895\n",
      "Iteration: 5300 Loss: 44645216.0217032\n",
      "Iteration: 5400 Loss: 44365511.28793757\n",
      "Iteration: 5500 Loss: 44089660.319156736\n",
      "Iteration: 5600 Loss: 43817523.343979396\n",
      "Iteration: 5700 Loss: 43548975.22924623\n",
      "Iteration: 5800 Loss: 43283903.52975623\n",
      "Iteration: 5900 Loss: 43022206.80421409\n",
      "Iteration: 6000 Loss: 42763793.16097494\n",
      "Iteration: 6100 Loss: 42508579.00215174\n",
      "Iteration: 6200 Loss: 42256487.93895463\n",
      "Iteration: 6300 Loss: 42007449.854840696\n",
      "Iteration: 6400 Loss: 41761400.096258275\n",
      "Iteration: 6500 Loss: 41518278.773536205\n",
      "Iteration: 6600 Loss: 41278030.15685426\n",
      "Iteration: 6700 Loss: 41040602.15429342\n",
      "Iteration: 6800 Loss: 40805945.86074218\n",
      "Iteration: 6900 Loss: 40574015.16797137\n",
      "Iteration: 7000 Loss: 40344766.427515656\n",
      "Iteration: 7100 Loss: 40118158.159141004\n",
      "Iteration: 7200 Loss: 39894150.79867139\n",
      "Iteration: 7300 Loss: 39672706.47979242\n",
      "Iteration: 7400 Loss: 39453788.845190175\n",
      "Iteration: 7500 Loss: 39237362.883020274\n",
      "Iteration: 7600 Loss: 39023394.785244174\n",
      "Iteration: 7700 Loss: 38811851.82484876\n",
      "Iteration: 7800 Loss: 38602702.249371655\n",
      "Iteration: 7900 Loss: 38395915.18850613\n",
      "Iteration: 8000 Loss: 38191460.57386629\n",
      "Iteration: 8100 Loss: 37989309.06925329\n",
      "Iteration: 8200 Loss: 37789432.00999201\n",
      "Iteration: 8300 Loss: 37591801.35010333\n",
      "Iteration: 8400 Loss: 37396389.616244614\n",
      "Iteration: 8500 Loss: 37203169.86749759\n",
      "Iteration: 8600 Loss: 37012115.66021032\n",
      "Iteration: 8700 Loss: 36823201.01720556\n",
      "Iteration: 8800 Loss: 36636400.40076482\n",
      "Iteration: 8900 Loss: 36451688.6888745\n",
      "Iteration: 9000 Loss: 36269041.15429597\n",
      "Iteration: 9100 Loss: 36088433.4460753\n",
      "Iteration: 9200 Loss: 35909841.57316606\n",
      "Iteration: 9300 Loss: 35733241.88987941\n",
      "Iteration: 9400 Loss: 35558611.08291779\n",
      "Iteration: 9500 Loss: 35385926.159778826\n",
      "Iteration: 9600 Loss: 35215164.43834857\n",
      "Iteration: 9700 Loss: 35046303.537523665\n",
      "Iteration: 9800 Loss: 34879321.36872919\n",
      "Iteration: 9900 Loss: 34714196.12821133\n",
      "Updated w:  [ 5350.41698532  1793.69496197  3275.85859686  1476.21988517\n",
      " 11237.64134256  1801.00601734]\n",
      "Updated b:  5472.846904532971\n"
     ]
    }
   ],
   "source": [
    "initial_w, initial_b, alpha = initialize_parameters()\n",
    "\n",
    "num_iters = 10000\n",
    "\n",
    "w,b,loss_hist = batch_gradient_descent(X_train ,y_train, initial_w, initial_b, alpha, num_iters)\n",
    "print(\"Updated w: \",w)\n",
    "print(\"Updated b: \",b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2f95d67-7992-4427-93d4-7d1ce3c6fc60",
   "metadata": {},
   "source": [
    "### Evaluating the Model\n",
    "We assess the model's accuracy using metrics like Mean Squared Error (MSE) and R-squared (RÂ²)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4ee713bb-673a-4d19-bd37-22d04d096a00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Error:  34552530.17315805 , Test Error:  34802633.04604892\n",
      "Train MSE:  69105060.34631617 , Test MSE:  69605266.09209786\n",
      "Train RÂ²:  0.5315475070962858 , Test RÂ²:  0.5145672428895158\n"
     ]
    }
   ],
   "source": [
    "train_error = loss_function(X_train, y_train, w, b)\n",
    "test_error = loss_function(X_test, y_test, w, b)\n",
    "print(\"Train Error: \",train_error, \", Test Error: \",test_error)\n",
    "\n",
    "def calculate_mse(y_true, y_pred):\n",
    "    return np.mean((y_true - y_pred) ** 2)\n",
    "\n",
    "def calculate_r2(y_true, y_pred):\n",
    "    ss_total = np.sum((y_true - np.mean(y_true)) ** 2)\n",
    "    ss_residual = np.sum((y_true - y_pred) ** 2)\n",
    "    return 1 - (ss_residual / ss_total)\n",
    "\n",
    "def predict(X, w, b):\n",
    "    return np.dot(X, w) + b\n",
    "\n",
    "y_train_pred = predict(X_train, w, b)\n",
    "y_test_pred = predict(X_test, w, b)\n",
    "\n",
    "train_mse = calculate_mse(y_train, y_train_pred)\n",
    "test_mse = calculate_mse(y_test, y_test_pred)\n",
    "\n",
    "train_r2 = calculate_r2(y_train, y_train_pred)\n",
    "test_r2 = calculate_r2(y_test, y_test_pred)\n",
    "\n",
    "print(\"Train MSE: \", train_mse, \", Test MSE: \", test_mse)\n",
    "print(\"Train RÂ²: \", train_r2, \", Test RÂ²: \", test_r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5edfa1d2-e748-4fbb-ba6d-2876e0680c53",
   "metadata": {},
   "source": [
    "### Analysing Model Coefficients\n",
    "We examine the model coefficients to understand the impact of each feature on the predicted\n",
    "charges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "170e5527-f972-44d6-9014-2f2c1ebf8419",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Coefficients:\n",
      "    Feature   Coefficient\n",
      "0    smoker  11237.641343\n",
      "1       age   5350.416985\n",
      "2       bmi   3275.858597\n",
      "3    region   1801.006017\n",
      "4       sex   1793.694962\n",
      "5  children   1476.219885\n",
      "\n",
      "Intercept (Bias): 5472.846904532971\n"
     ]
    }
   ],
   "source": [
    "def display_coefficients(features, coefficients, intercept):\n",
    "    coef_df = pd.DataFrame({\n",
    "        'Feature': features,\n",
    "        'Coefficient': coefficients\n",
    "    })\n",
    "    coef_df = coef_df.sort_values(by='Coefficient', ascending=False)\n",
    "    coef_df = coef_df.reset_index(drop=True)\n",
    "    \n",
    "    print(\"\\nModel Coefficients:\")\n",
    "    print(coef_df)\n",
    "    print(\"\\nIntercept (Bias):\", intercept)\n",
    "\n",
    "feature_names = df.drop(columns=['charges']).columns.tolist()\n",
    "\n",
    "display_coefficients(feature_names, w, b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7258a383-8168-401b-911d-d40cafd70c3d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
